{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2: Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "dir(models);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetV2 = models.MobileNetV2()\n",
    "mobilenetV2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.AlexNet()\n",
    "# we will run a forward pass through the network\n",
    "resnet = models.resnet101(pretrained=True)\n",
    "resnet;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we’ll see a lot of Bottleneck modules repeating one after the other (101 of them!), containing convolutions and other modules. That’s the anatomy(组成部分) of a typical deep neural network for computer vision: a more or less sequential cascade of filters and nonlinear functions, ending with a layer (fc) producing scores for each of the 1,000 output classes (out_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picture preprocessing \n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "])\n",
    "\n",
    "img = Image.open(\"../data/p1ch2/bobby.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([1, 3, 224, 224]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t = preprocess(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "img_t.shape, batch_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('golden retriever', 96.29336547851562)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.eval()\n",
    "out = resnet(batch_t)\n",
    "torch.argmax(out).item()\n",
    "with open('../data/p1ch2/imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "_, index = torch.max(out, 1)\n",
    "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "labels[index[0]], percentage[index[0]].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('golden retriever', 96.29336547851562),\n",
       " ('Labrador retriever', 2.8081140518188477),\n",
       " ('cocker spaniel, English cocker spaniel, cocker', 0.28267380595207214),\n",
       " ('redbone', 0.20863007009029388),\n",
       " ('tennis ball', 0.11621550470590591)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, indices = torch.sort(out, descending=True)\n",
    "[(labels[idx], percentage[idx].item()) for idx in indices[0][:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet (http://mng.bz/lo6z), ResNet (https://arxiv.org/pdf/1512.03385.pdf), and Inception v3 (https://arxiv.org/pdf/1512.00567.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3 : It starts with a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning really consists of building a system that can transform data from one representation to another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we must first have a solid understanding of how PyTorch handles and stores data—as input, as intermediate representations, and as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is also called multidimensional array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]), tensor(1.), 1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3)\n",
    "a, a[1], float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2] = 2.0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]),\n",
       " torch.Size([3, 2]),\n",
       " tensor(1.),\n",
       " torch.Size([]),\n",
       " torch.Size([3]),\n",
       " tensor([4., 1.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points, points.shape, points[0, 1], points[0, 1].shape, points[:, 0].shape, points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor([[0., 0., 0.]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.zeros(3, 2)\n",
    "point_column = points[:, 0]\n",
    "point_column_unsqueeze = point_column.unsqueeze(0)\n",
    "point_column_unsqueeze2 = point_column.unsqueeze(1)\n",
    "print(point_column)\n",
    "print(point_column_unsqueeze)\n",
    "print(point_column_unsqueeze2)\n",
    "point_column.shape, point_column_unsqueeze.shape, point_column_unsqueeze2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3, 4, 5])\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "x =  torch.tensor(range(6)) # tensor([0,1,2,3,4,5])\n",
    "print(x)\n",
    "print(x[:]) # All elements in the list\n",
    "print(x[1:4]) # From element 1 inclusive to element 4 exclusive\n",
    "print(x[1:]) # From element 1 inclusive to the end of the list\n",
    "print(x[:4]) # From the start of the list to element 4 exclusive\n",
    "print(x[:-1]) # From the start of the list to one before the last element\n",
    "print(x[1:4:2]) # From element 1 inclusive to element 4 exclusive, in stemps of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([[5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([[5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([5., 2.])\n",
      "torch.Size([1, 3, 2])\n",
      "torch.Size([3, 1, 2])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "print(points)\n",
    "print(points[1:]) # All row after the first; implicitly all columns\n",
    "print(points[1:, :]) # All rows after the first; all columns\n",
    "print(points[1:, 0]) # All rows after the first; first column\n",
    "print(points[None].shape) # Adds a dimension of size 1, just like unsqueeze\n",
    "print(points[:,None].shape)\n",
    "print(points[:,:,None].shape)\n",
    "# print(points[:,:,:,None].shape) error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5)  # shape [channels, rows, columns]\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sometimes the RGB channels are in dimension 0, and sometimes they are in dimen- sion 1. But we can generalize by counting from the end: they are always in [dimension –3](), the third from the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights size:          torch.Size([3, 1, 1])\n",
      "img_weigthts size:     torch.Size([3, 5, 5])\n",
      "batch_weights size: torch.Size([2, 3, 5, 5])\n",
      "img_gray_weighted size:   torch.Size([5, 5])\n",
      "batch_gray_weighted:   torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "print(\"weights size:         \", unsqueezed_weights.shape)\n",
    "print(\"img_weigthts size:    \", img_weights.shape)\n",
    "print(\"batch_weights size:\", batch_weights.shape)\n",
    "print('img_gray_weighted size:  ', img_gray_weighted.shape)\n",
    "print(\"batch_gray_weighted:  \", batch_gray_weighted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)\\\n",
    "Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\\\n",
    "Image  (3d array): 256 x 256 x 3\\\n",
    "Scale  (1d array):             3\\\n",
    "Result (3d array): 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0]],\n",
       " \n",
       "         [[1]],\n",
       " \n",
       "         [[1]]]),\n",
       " tensor([[1, 1],\n",
       "         [0, 1]]),\n",
       " tensor([[[1, 1],\n",
       "          [0, 1]],\n",
       " \n",
       "         [[2, 2],\n",
       "          [1, 2]],\n",
       " \n",
       "         [[2, 2],\n",
       "          [1, 2]]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(2, (3, 1, 1))\n",
    "y = torch.randint(2,(2,2))\n",
    "x, y, x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caoyi/opt/anaconda3/envs/torchenv/lib/python3.9/site-packages/torch/_tensor.py:880: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1646756029501/work/c10/core/TensorImpl.h:1463.)\n",
      "  return super(Tensor, self).refine_names(names)\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int16"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = torch.tensor(range(6), dtype=torch.short)\n",
    "indexes[indexes[0]].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float64,\n",
       " torch.int16,\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]], dtype=torch.float64),\n",
       " tensor([[1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1]], dtype=torch.int16))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_points = torch.ones(10, 2, dtype=torch.double)\n",
    "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n",
    "double_points = torch.zeros(10, 2).to(torch.double)\n",
    "short_points = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_points.dtype, short_points.dtype, double_points, short_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7086, 0.0760, 0.1358, 0.8451, 0.5574], dtype=torch.float64),\n",
       " tensor([0, 0, 0, 0, 0], dtype=torch.int16),\n",
       " tensor([0., 0., 0., 0., 0.], dtype=torch.float64))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = torch.rand(5, dtype=torch.double)\n",
    "points_short = points_64.to(torch.short)\n",
    "points_64, points_short, points_64 * points_short  # works from PyTorch 1.3 onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tensor API [torch module]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of operations on and between tensors are available in the torch module and can also be called as methods of a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "a_t_t = a_t.transpose(0, 1)\n",
    "a.shape, a_t.shape, a_t_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors: Scenic views of storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_storage = points.storage()\n",
    "points_storage[0], points_storage[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_storage = points.storage()\n",
    "points_storage[0] = 2.0\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In-place operations\n",
    "a = torch.ones(3, 2)\n",
    "a.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([2]), torch.Size([2]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor metadata: Size, offset, and stride\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1]\n",
    "second_point.storage_offset(), second_point.size(), second_point.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing an element i, j in a 2D tensor results in accessing the storage_offset + stride[0] * i + stride[1] * j element in the storage. The offset will usually be zero; if this tensor is a view of a storage created to hold a larger tensor, the offset might be a positive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 7 : Telling birds from airplanes : Learning from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)\n",
    "\n",
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "tensor_cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)\n",
    "\n",
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in tensor_cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in tensor_cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = cifar2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 8 : Using convolutions to generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fully connected setup needed to detect the various possible translations of the bird or airplane in the image, we have both too many parameters (making it easier for the model to memorize the training set) and no position independence (making it harder to generalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w00, w01, w02, w10, w11, w12, w20, w21, w22 = torch.zeros(9)\n",
    "weight = torch.tensor([[w00, w01, w02],\n",
    "                       [w10, w11, w12],\n",
    "                       [w20, w21, w22]])\n",
    "\n",
    "image = torch.tensor([[i00, i01, i02, i03, ..., i0N],\n",
    "                      [i10, i11, i12, i13, ..., i1N],\n",
    "                      [i20, i21, i22, i23, ..., i2N],\n",
    "                      [i30, i31, i32, i33, ..., i3N],\n",
    "                      ...\n",
    "                      [iM0, iM1m iM2, iM3, ..., iMN]])\n",
    "\n",
    "o11 = i11*w00 + i12*w01 + i13*w02 +\n",
    "      i21*w10 + i22*w11 + i23*w12 +\n",
    "      i31*w20 + i32*w21 + i33*w22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we “translate” the [kernel]() on the i11 location of the input image, and we\n",
    "multiply each weight by the value of the input image at the corresponding location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multichannel image, like our RGB image, the weight matrix would be a [3 × 3 × 3]() matrix: one set of weights for every channel, contributing together to the output values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing, by switching to convolutions, we get \n",
    "* Local operations on neighborhoods\n",
    "* Translation invariance\n",
    "* Models with a lot fewer parameters\\\n",
    "The key insight underlying the third point is that, with a convolution layer, the num- ber of parameters depends not on the number of pixels in the image, as was the case in our fully connected model, but rather on the size of the convolution kernel (3 × 3, 5 × 5, and so on) and on how many convolution filters (or output channels) we decide to use in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single output pixel value, our kernel would consider, say, in_ch = 3 input channels, so the weight component for a single output pixel value (and by translation the invariance for the entire output channel) is of shape in_ch × 3 × 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the complete weight tensor is out_ch × in_ch × 3 × 3, in our case 16 × 3 × 3 × 3. The bias will have size 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we need to add the zeroth batch dimension with unsqueeze if we want to call the conv module with one input image, since nn.Conv2d expects a B × C × H × W shaped tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c71e970b376e5910de56d332af36cb508b823811ace826c2f1193ceeb2dcc3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
