{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Residual Learning for Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* problem : deeper \"plain\" network has higher training error, and thus test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are [identity mapping](), and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Residual Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Residual Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $H(x)$ : an underlying mapping to be fit by a few stacked layers (not necessarily the entire net)\n",
    "* $x$ : the inputs to the first of these layers\n",
    "* it is equivalent to hypothesize that they can asymptotically approximate the [residual functions](), i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).\n",
    "* we explicitly let these layers approximate a residual function $F(x) := H(x) − x$. The original function thus becomes $F(x) + x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![residual learning building block](../imgs/resnet_fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this paper we consider a building block defined as: $y = F(x, \\{W_{i}\\}) + x$\n",
    "* For the example in Fig. 2 that has two layers, $F = W_{2} \\cdot σ(W_{1} \\cdot x)$ $σ$ denotes ReLU\n",
    "* We adopt the second nonlinearity after the addition (i.e., $σ(y)$, see Fig. 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c71e970b376e5910de56d332af36cb508b823811ace826c2f1193ceeb2dcc3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
