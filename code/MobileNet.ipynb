{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[streamlined architecture ?]() that uses [depth-wise separable ?]() convolutions to build [light weight ?]() deep neural networks.\\\n",
    "[Two simple global hyperparameters ?]() trade off between [latency (delay ?)](https://www.techtarget.com/whatis/definition/latency) and accuracy.\\\n",
    "These hyper-parameters allow us to choose the right sized model !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The main difference between 2D convolutions and [Depthwise Convolution]() is that 2D convolutions are performed over all/multiple input channels, whereas in Depthwise convolution, each channel is kept separate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet(2012) win ImageNet Challenge: ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012.\n",
    "\n",
    "Problem: the recognition tasks need to be carried out in a timely fashion on a computationally limited platform.\n",
    "\n",
    "Notes: i.e. need to become more effciency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet can be easily matched to the design requirements for [mobile and embedded vision applications]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prior work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with small networks of other papers, MobileNets optimize latency and also yield networks with small size.\n",
    "\n",
    "MobileNets are built primarily from depthwise separable convolutions initially introduced in [Rigid-motion scattering for image classification](../papers/phd_sifre.pdf) and subsequently used in [Inception models](../papers/BatchNomalization.pdf) to reduce the computation in the first few layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blog: [Understanding Depthwise Separable Convolutions and the efficiency of MobileNets](../blogs/DepthwiseSeparableConvolutions.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MobileNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first describe the core layers that Mo- bileNet is built on which are depthwise separable filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Depthwise Separable Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into [a depthwise convolution]() and a 1 × 1 convolution called [a pointwise convolution]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "# cifar10 = datasets.CIFAR10(data_path, train=True, download=False)\n",
    "# cifar10_val = datasets.CIFAR10(data_path, train=False, download=False)\n",
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "tensor_cifar10_val = datasets.CIFAR10(data_path, train=False, download=False)\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in tensor_cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in tensor_cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[torch Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 3, 3]), torch.Size([6]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depthwise convolution\n",
    "Cin, k = 3, 2\n",
    "Cout = k * Cin\n",
    "depthwise_conv = nn.Conv2d(Cin, Cout, kernel_size=3, groups=Cin, padding='same')\n",
    "depthwise_conv.weight.shape, depthwise_conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 32, 32]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# depthwise convolution\n",
    "img, _ = cifar2[0]\n",
    "img = img.unsqueeze(0)\n",
    "depthwise_output = depthwise_conv(img)\n",
    "img.shape, depthwise_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3, 1, 1]), torch.Size([1, 10, 32, 32]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 x 1 convolution / pointwise convolution\n",
    "Cin = 3\n",
    "Cout = 10\n",
    "pointwise_conv = nn.Conv2d(Cin, Cout, kernel_size=1)\n",
    "pointwise_output = pointwise_conv(img)\n",
    "pointwise_conv.weight.shape, pointwise_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* standard convolutional layer :\n",
    "* input : $M \\times D_{F} \\times D_{F}$, $M$ is input depth, $D_{F}$ is the spatial width and height of a square input feature map $F$\n",
    "* convolution kernel $K$ : $N \\times M \\times D_{K} \\times D_{K}$, $N$ is output depth, $M$ is input depth, $D_{K}$ is the spatial dimension of the square kernel\n",
    "* output : $N \\times D_{G} \\times D_{G}$, $N$ is output depth, $D_{G}$ isthe spatial width and height of a square output feature map $G$\n",
    "* $G_{n, k, l} = \\sum_{m, i, j} K_{n, m, i, j} \\cdot F_{m, k+i-1, l+j-1}$\n",
    "* computational cost : $N \\cdot M \\cdot D_{K} \\cdot D_{K} \\cdot D_{F} \\cdot D_{F}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Depthwise convolution :\n",
    "* input : $M \\times D_{F} \\times D_{F}$ feature map $F$\n",
    "* kernel : $M \\times 1 \\times D_{K} \\times D_{K}$\n",
    "* output : $M \\times D_{G} \\times D_{G}$ feature map $G$\n",
    "* no padding, stride 1 example : input[3, 32, 32] -> kernel[3, 1, 3, 3] -> output[3, 30, 30]\n",
    "* computational cost : $M \\cdot 1 \\cdot D_{K} \\cdot D_{K} \\cdot D_{F} \\cdot D_{F}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $1 \\times 1$ (pointwise) convolution\n",
    "* compute a linear combination of the output of depthwise convolution\n",
    "* input from depthwise convolution: $M \\times D_{G} \\times D_{G}$\n",
    "* kernel : $N \\times M \\times 1 \\times 1$\n",
    "* output : $N \\times D_{G} \\times D_{G}$\n",
    "* computational cost : $N \\cdot M \\cdot 1 \\cdot 1 \\cdot D_{G} \\cdot D_{G}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Network Structure and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model structure puts nearly all of the computation into dense 1 × 1 convolutions. This can be implemented with highly optimized ***general matrix multiply (GEMM)*** functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we found that it was important to put very little or no weight decay (l2 regularization) on the depthwise filters since their are so few parameters in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Width Multiplier: Thinner Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given layer and width multiplier α, the number of input channels M be- comes αM and the number of output channels N becomes αN. ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Resolution Multiplier: Reduced Representa- tion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ρ ∈ (0, 1] which is typically set implicitly so that\n",
    "the input resolution of the network is 224, 192, 160 or 128. ρ = 1 is the baseline MobileNet and ρ < 1 are reduced computation MobileNets. Resolution multiplier has the ef- fect of reducing computational cost by ρ2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c71e970b376e5910de56d332af36cb508b823811ace826c2f1193ceeb2dcc3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
